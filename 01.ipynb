{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "import datetime as dt\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "#This file reads data from honeypot dataset, process and write the data into bots.txt and humans.txt\n",
    "\n",
    "num_decimal = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading basic info for each user into container\n",
    "#account id ----\n",
    "def read_info(filename, container,container2):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            tokens = re.split(\"[\\t\\n]\", line)\n",
    "            \n",
    "            #Parse strings to numbers, take out the dates\n",
    "            parsed_tokens = [tokens[0]] + [int(r) for r in tokens[3:8]]\n",
    "            #Remove length of screen name from data, which is found to have negative effect on classification accuracy\n",
    "            parsed_tokens = parsed_tokens[:4] + parsed_tokens[6:]\n",
    "            #Add # followings / # followers\n",
    "            parsed_tokens.append(round((parsed_tokens[1]/parsed_tokens[2] if parsed_tokens[2] else 0), num_decimal))\n",
    "            container.append(parsed_tokens)\n",
    "            container2.append([tokens[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_followings(filename, container):\n",
    "    with open(filename, 'r') as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            tokens = [int(r) for r in re.split(\"[\\D]\", line) if r != \"\"][1:]\n",
    "            #Calculate standard deviation of the data\n",
    "            sd = round(stat.pstdev(tokens), num_decimal)\n",
    "            #Calcalate standard deviation of differences of the data\n",
    "            sdd = round(stat.pstdev(list(np.array(tokens[1:]) - np.array(tokens[:-1]))), num_decimal)\n",
    "            #Calculate lag one autocorrelation of the data\n",
    "            avg = np.mean(tokens)\n",
    "            numerator = sum((np.array(tokens[1:]) - avg) * (np.array(tokens[:-1]) - avg))\n",
    "            denominator = sum((np.array(tokens) - avg) ** 2)\n",
    "            lac = round(numerator/denominator, num_decimal) if denominator != 0 else 0\n",
    "            container[i] += [sd, sdd, lac]\n",
    "            i += 1\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA------------------\n",
    "def read_texts(filename,container):\n",
    "    curr_userID = \"\"\n",
    "    contents = \"\"\n",
    "    i = 0\n",
    "    \n",
    "    with open(filename, encoding = 'utf-8', mode = 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            tokens = [r for r in re.split(\"[\\t\\n]\", line) if r != \"\"]\n",
    "            if not line or tokens[0] != curr_userID and curr_userID:\n",
    "                #New user found / eof reached\n",
    "                while curr_userID != container[i][0]:\n",
    "                    i += 1  \n",
    "                container[i].append(contents)\n",
    "                if not line:\n",
    "                    break\n",
    "                contents = \"\"\n",
    "            curr_userID = tokens[0]\n",
    "            \n",
    "            curr_content=tokens[2]\n",
    "            # print(curr_userID)\n",
    "            # print(curr_content)\n",
    "            # print(tokens[1])\n",
    "            contents+=curr_content\n",
    "            \n",
    "           \n",
    "                \n",
    "            #Reading tweets posted by each user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading tweets posted by each user\n",
    "def read_tweets(filename, container):\n",
    "    curr_userID = \"\"\n",
    "    curr_tweet_count = 0\n",
    "    #Features contained in tweets (current user)\n",
    "    urls, _at_s, hashtags, weekday_post = [], [], [], []\n",
    "    #Index for container\n",
    "    i = 0\n",
    "    with open(filename, encoding = 'utf-8', mode = 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            tokens = [r for r in re.split(\"[\\t\\n]\", line) if r != \"\"]\n",
    "            if not line or tokens[0] != curr_userID and curr_userID:\n",
    "                #New user found / eof reached\n",
    "                num_tweets_weekday = [weekday_post.count(i) for i in range(7)]\n",
    "                ratio_tweets_weekday = [round(x, num_decimal) for x in list(np.array(num_tweets_weekday) / len(weekday_post))]\n",
    "                curr_user = num_tweets_weekday + ratio_tweets_weekday\n",
    "                for feature in (urls, _at_s, hashtags):\n",
    "                    curr_user.append(round(len(feature) / curr_tweet_count, num_decimal))\n",
    "                    curr_user.append(round(len(set(feature)) / curr_tweet_count, num_decimal))\n",
    "                while curr_userID != container[i][0]:\n",
    "                    i += 1\n",
    "                container[i] += curr_user\n",
    "                if not line:\n",
    "                    break\n",
    "                #Reset current user info containers\n",
    "                curr_tweet_count = 0\n",
    "                urls, _at_s, hashtags, weekday_post = [], [], [], []\n",
    "            #Post date of the tweet\n",
    "            curr_userID = tokens[0]\n",
    "            curr_tweet_count += 1\n",
    "            # content\n",
    "            curr_tweet_count += 1\n",
    "            urls += re.findall('http[\\S]+', tokens[2])\n",
    "            _at_s += re.findall('@[\\S]+', tokens[2])\n",
    "            hashtags += re.findall('#[\\S]+', tokens[2])\n",
    "            post_date = re.split(\"[-\\s]\", tokens[3])\n",
    "            post_date = dt.date(int(post_date[0]), int(post_date[1]), int(post_date[2]))\n",
    "            weekday_post.append(post_date.weekday())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting ambiguous users who are in both polluters and legitimate users (44 found)\n",
    "#The user ids are found in ascending order\n",
    "def del_amb(bots, humans):\n",
    "    i, j, count = (0, 0, 0)\n",
    "    while i < len(bots) and j < len(humans):\n",
    "        if bots[i][0] == humans[j][0]:\n",
    "            bots.pop(i)\n",
    "            humans.pop(j)\n",
    "            count += 1\n",
    "        elif int(bots[i][0]) < int(humans[j][0]):\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return count\n",
    "\n",
    "#Add 0's for missing values (some users have no tweets recorded)\n",
    "def add0(container):\n",
    "    length = 29\n",
    "    for i in range(len(container)):\n",
    "        container[i] += [0]*(length - len(container[i]))\n",
    "        \n",
    "def add00(container):\n",
    "    length = 2\n",
    "    for i in range(len(container)):\n",
    "        container[i] += [0]*(length - len(container[i]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write data into text files\n",
    "def write_user(filename, container):\n",
    "    with open(filename, 'w') as f:\n",
    "        for inst in container:\n",
    "            f.write(\"\\t\".join([str(x) for x in inst]))\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('social_honeypot_icwsm_2011'):\n",
    "    if not os.path.exists('social_honeypot_icwsm_2011.zip'):\n",
    "        print(\"downloading data\")\n",
    "        urlretrieve('http://infolab.tamu.edu/static/users/kyumin/social_honeypot_icwsm_2011.zip', 'social_honeypot_icwsm_2011.zip')\n",
    "    zip = zipfile.ZipFile('social_honeypot_icwsm_2011.zip')\n",
    "    zip.extractall(path = 'social_honeypot_icwsm_2011')\n",
    "    zip.close()\n",
    "    print(\"data ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing\n",
      "data read from social_honeypot_icwsm_2011/content_polluters.txt\n",
      "data read from social_honeypot_icwsm_2011/legitimate_users.txt\n",
      "data read from social_honeypot_icwsm_2011/content_polluters_tweets.txt\n",
      "data read from social_honeypot_icwsm_2011/legitimate_users_tweets.txt\n",
      "44 mislabeled users deleted!\n",
      "added 0's for missing values\n",
      "data written to botst.txt\n",
      "data written to humanst.txt\n",
      "data written to bots.txt\n",
      "data written to humans.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"executing\")\n",
    "bots, humans,botst,humanst = [], [],[],[]\n",
    "read_info('social_honeypot_icwsm_2011/social_honeypot_icwsm_2011/content_polluters.txt', bots, botst)\n",
    "print(\"data read from social_honeypot_icwsm_2011/content_polluters.txt\")\n",
    "read_info('social_honeypot_icwsm_2011/social_honeypot_icwsm_2011/legitimate_users.txt', humans,humanst)\n",
    "print(\"data read from social_honeypot_icwsm_2011/legitimate_users.txt\")\n",
    "\n",
    "read_texts('social_honeypot_icwsm_2011/social_honeypot_icwsm_2011/content_polluters_tweets.txt', botst)\n",
    "print(\"data read from social_honeypot_icwsm_2011/content_polluters_tweets.txt\")\n",
    "read_texts('social_honeypot_icwsm_2011/social_honeypot_icwsm_2011/legitimate_users_tweets.txt', humanst)\n",
    "print(\"data read from social_honeypot_icwsm_2011/legitimate_users_tweets.txt\")\n",
    "'''\n",
    "read_followings('social_honeypot_icwsm_2011/social_honeypot_icwsm_2011/content_polluters_followings.txt', bots)\n",
    "print(\"data read from social_honeypot_icwsm_2011\\content_polluters_followings.txt\")\n",
    "read_followings('social_honeypot_icwsm_2011/social_honeypot_icwsm_2011/legitimate_users_followings.txt', humans)\n",
    "print(\"data read from social_honeypot_icwsm_2011\\legitimate_users_followings.txt\")\n",
    "\n",
    "read_tweets('social_honeypot_icwsm_2011/social_honeypot_icwsm_2011/legitimate_users_tweets.txt', humans)\n",
    "print(\"data read from social_honeypot_icwsm_2011/legitimate_users_tweets.txt\")\n",
    "read_tweets('social_honeypot_icwsm_2011/social_honeypot_icwsm_2011/content_polluters_tweets.txt', bots)\n",
    "print(\"data read from social_honeypot_icwsm_2011/content_polluters_tweets.txt\")\n",
    "'''\n",
    "count = del_amb(bots, humans)\n",
    "print(\"%d mislabeled users deleted!\" % count)\n",
    "add0(bots)\n",
    "add0(humans)\n",
    "add00(botst)\n",
    "add00(humanst)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "print(\"added 0's for missing values\")\n",
    "\n",
    "write_user('botst.txt',botst)\n",
    "print(\"data written to botst.txt\")\n",
    "write_user('humanst.txt', humanst)\n",
    "print(\"data written to humanst.txt\")\n",
    "\n",
    "write_user('bots.txt', bots)\n",
    "print(\"data written to bots.txt\")\n",
    "write_user('humans.txt', humans)\n",
    "print(\"data written to humans.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n"
     ]
    }
   ],
   "source": [
    "botdic = {}\n",
    "print(\"executed\")\n",
    "with open(\"botst.txt\", 'r') as f:\n",
    "    s = f.readline()\n",
    "    while s:\n",
    "        p=s.split(\"\t\")\n",
    "        botdic[p[0]]=p[1]\n",
    "        s = f.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n"
     ]
    }
   ],
   "source": [
    "humandic = {}\n",
    "print(\"executed\")\n",
    "with open(\"humanst.txt\", 'r') as f:\n",
    "    s = f.readline()\n",
    "    while s:\n",
    "        p=s.split(\"\t\")\n",
    "        humandic[p[0]]=p[1]\n",
    "        s = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-5c3741dd0f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# print(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# convert to lower case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     return [\n\u001b[1;32m    145\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m    104\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \"\"\"\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \"\"\"\n\u001b[0;32m-> 1331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \"\"\"\n\u001b[0;32m-> 1331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[1;32m   1361\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install nltk\n",
    "\n",
    "\n",
    "import nltk\n",
    "# nltk.download('all')\n",
    "print(\"executed\")\n",
    "for key in botdic:\n",
    "    text = botdic[key]\n",
    "    # print(botdic[key])\n",
    "    # split into words\n",
    "    # print(\"before\")\n",
    "    # print(text)\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in tokens]\n",
    "    \n",
    "    botdic[key]=stemmed\n",
    "# print('afterprocessing:')   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-21a4e25e2049>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# print(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# convert to lower case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     return [\n\u001b[1;32m    145\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m    104\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \"\"\"\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \"\"\"\n\u001b[0;32m-> 1331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \"\"\"\n\u001b[0;32m-> 1331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[1;32m   1361\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('all')\n",
    "print(\"executed\")\n",
    "for key in humandic:\n",
    "    text = humandic[key]\n",
    "    # print(botdic[key])\n",
    "    # split into words\n",
    "    # print(\"before\")\n",
    "    # print(text)\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in tokens]\n",
    "    \n",
    "    humandic[key]=stemmed\n",
    "# print('afterprocessing:')   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.013953499), (1, 0.013953499), (2, 0.013953499), (3, 0.013953499), (4, 0.014064783), (5, 0.013953499), (6, 0.013953499), (7, 0.013953499), (8, 0.013953499), (9, 0.80351615), (10, 0.013953499), (11, 0.013953499), (12, 0.013953499), (13, 0.013953499), (14, 0.01497709)]\n"
     ]
    }
   ],
   "source": [
    "#lda distribution\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "\n",
    "# print(botdic[\"788352\"])\n",
    "# Train the model on the corpus.\n",
    "lda = gensim.models.ldamodel.LdaModel(common_corpus, num_topics=15, alpha=0.3,eta=0.01)  # learn asymmetric alpha from data\n",
    "bot_corpus={}\n",
    "for text in botdic:\n",
    "    dist = common_dictionary.doc2bow(botdic[text])\n",
    "    bot_corpus[text]=lda[dist]\n",
    "#bot_corpus = [common_dictionary.doc2bow(botdic[text]) for text in botdic]\n",
    "#human_corpus = [common_dictionary.doc2bow(humandic[text]) for text in humandic]\n",
    "human_corpus={}\n",
    "for text in humandic:\n",
    "    dist = common_dictionary.doc2bow(humandic[text])\n",
    "    human_corpus[text]=lda[dist]\n",
    "# print(lda[other_corpus])\n",
    "\n",
    "print(bot_corpus[\"788352\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 0.8839612)]\n"
     ]
    }
   ],
   "source": [
    "print(bot_corpus[\"15742514\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22223\n"
     ]
    }
   ],
   "source": [
    "print(len(bot_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOSS AND LOSS \n",
    "#GOSS \n",
    "def goss_val(corpus):\n",
    "    import math\n",
    "    goss =[]\n",
    "    from collections import defaultdict\n",
    "\n",
    "    goss = defaultdict(dict)\n",
    "\n",
    "\n",
    "    for k in range(15):\n",
    "        u=0\n",
    "        d=0\n",
    "        for i in corpus:\n",
    "            for j,p in corpus[i]:\n",
    "                if j==k:\n",
    "                    #u+=bot_corpus[i][k][1]\n",
    "                    u+=p\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "        u/=len(corpus)\n",
    "        for i in corpus:\n",
    "            for j,p in corpus[i]:\n",
    "                if j==k:\n",
    "                    #u+=bot_corpus[i][k][1]\n",
    "                    d+=(p-u)*(p-u)\n",
    "                    break\n",
    "            '''\n",
    "            if bot_corpus[i][j]:\n",
    "                for j in range(k):\n",
    "                    if bot_corpus[i][j][0]==k:\n",
    "                        d+=(bot_corpus[i][j][1]-u)*(bot_corpus[i][j][1]-u)\n",
    "                        break\n",
    "                    j+=1\n",
    "            '''\n",
    "\n",
    "        math.sqrt(d)\n",
    "        for i in corpus:\n",
    "            '''\n",
    "            if bot_corpus[i][j]:\n",
    "                for j in range(k):\n",
    "                    if bot_corpus[i][j][0]==k:\n",
    "                        goss[i][k] = (bot_corpus[i][j][1]-u)/d\n",
    "                        break\n",
    "                    j+=1\n",
    "            '''\n",
    "            for j,p in corpus[i]:\n",
    "                if j==k:\n",
    "                    #u+=bot_corpus[i][k][1]\n",
    "                    goss[i][k] = (p-u)/d\n",
    "                    break\n",
    "                else: goss[i][k]=0.00\n",
    "    print(\"goss for this corpus is calculated\")\n",
    "    return goss\n",
    "\n",
    "    #print(goss[\"788352\"])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goss for this corpus is calculated\n",
      "goss for this corpus is calculated\n"
     ]
    }
   ],
   "source": [
    "human_goss=goss_val(human_corpus)\n",
    "bot_goss = goss_val(bot_corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOSS\n",
    "def loss_val(corpus):\n",
    "    import math\n",
    "    goss =[]\n",
    "    from collections import defaultdict\n",
    "\n",
    "    loss = defaultdict(dict)\n",
    "\n",
    "\n",
    "    for i in corpus:\n",
    "        u=0\n",
    "        d=0\n",
    "        for k in range(15):\n",
    "            for j,p in corpus[i]:\n",
    "                if j==k:\n",
    "                    #u+=bot_corpus[i][k][1]\n",
    "                    u+=p\n",
    "                    break\n",
    "        u/=15\n",
    "        for k in range(15):\n",
    "            for j,p in corpus[i]:\n",
    "                if j==k:\n",
    "                    #u+=bot_corpus[i][k][1]\n",
    "                    d+=(p-u)*(p-u)\n",
    "                    break\n",
    "            '''\n",
    "            if bot_corpus[i][j]:\n",
    "                for j in range(k):\n",
    "                    if bot_corpus[i][j][0]==k:\n",
    "                        d+=(bot_corpus[i][j][1]-u)*(bot_corpus[i][j][1]-u)\n",
    "                        break\n",
    "                    j+=1\n",
    "            '''\n",
    "\n",
    "        math.sqrt(d)\n",
    "        for k in range(15):\n",
    "            '''\n",
    "            if bot_corpus[i][j]:\n",
    "                for j in range(k):\n",
    "                    if bot_corpus[i][j][0]==k:\n",
    "                        goss[i][k] = (bot_corpus[i][j][1]-u)/d\n",
    "                        break\n",
    "                    j+=1\n",
    "            '''\n",
    "            for j,p in corpus[i]:\n",
    "                if j==k:\n",
    "                    #u+=bot_corpus[i][k][1]\n",
    "                    loss[i][k] = (p-u)/d\n",
    "                    break\n",
    "                else: loss[i][k]=0.00\n",
    "    print(\"loss for this corpus is calculated\")\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for this corpus is calculated\n",
      "loss for this corpus is calculated\n"
     ]
    }
   ],
   "source": [
    "human_loss=loss_val(human_corpus)\n",
    "bot_loss = loss_val(bot_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0013005006667086874\n",
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 1.2120764696041115, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(bot_goss[\"788352\"][0])\n",
    "print(bot_loss[\"15742514\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_gl = {}\n",
    "for i in bot_goss:\n",
    "    human_gl[i] = bot_goss[i]\n",
    "    human_gl[i]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_list(dict):\n",
    "    dic={}\n",
    "    for i in dict:\n",
    "        dic[i]=[]\n",
    "        for value in dict[i]:\n",
    "            dic[i].append(dict[i][value])\n",
    "    print(\"converted\")\n",
    "    return dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted\n",
      "converted\n",
      "converted\n",
      "converted\n"
     ]
    }
   ],
   "source": [
    "bot_gos = dict_list(bot_goss)\n",
    "bot_los = dict_list(bot_loss)\n",
    "human_gos = dict_list(human_goss)\n",
    "human_los = dict_list(human_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0006215511769755345, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(bot_gos[\"15742514\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "#loss\n",
    "#let human = 1, bot = 0\n",
    "loss_set = []\n",
    "loss_target = []\n",
    "\n",
    "for i in human_los:\n",
    "    #for k in human_loss[i]:\n",
    "    loss_set.append(human_los[i])\n",
    "    loss_target.append(1)\n",
    "\n",
    "for j in bot_los:\n",
    "    # for m in bot_loss[j]:\n",
    "    loss_set.append(bot_los[j])\n",
    "    loss_target.append(0.00)\n",
    "        \n",
    "goss_set = []\n",
    "goss_target = []\n",
    "for i in human_gos:\n",
    "    # for k in human_goss[i]:\n",
    "    goss_set.append(human_gos[i])\n",
    "    goss_target.append(1)    \n",
    "for j in bot_gos:\n",
    "    # for m in bot_goss[j]:\n",
    "    goss_set.append(bot_gos[j])\n",
    "    goss_target.append(0.00)\n",
    " #loss_goss_set = defaultdict(dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0007240364726450916, -4.58463984862702e-05, 0.0012004149478232339, 0.001200414954446007, -0.0001052071895827132, 0.0012004149544390505, 0.001197882486843453, 0.0009515685166071788, 0.001200414959833163, -7.09759969684995e-05, 0.0012004148215825155, 0.0012004148888977004, 0.0012004149425423052, 0.001200414721438811, 9.23785369020726e-05]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(goss_set[1])\n",
    "print(goss_target[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63695\n",
      "63695\n"
     ]
    }
   ],
   "source": [
    "print(len(goss_set))\n",
    "print(len(goss_target))\n",
    "# print(loss_set[25426])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 filled\n",
      "(38208, 15)\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Classifier: SVM \n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import svm\n",
    "    # human: X; bot: Y\n",
    "    los_set = np.array(loss_set)\n",
    "    np.nan_to_num(los_set)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    los_set,loss_target, test_size=0.4, random_state=0)\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        while len(X_train[i])!= 15:\n",
    "            X_train[i].append(0)\n",
    "            print(X_train[i])\n",
    "    print(\"0 filled\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        for j in range(len(X_train[i])):\n",
    "            if np.isnan(X_train[i][j]) or not np.isfinite(X_train[i][j]):\n",
    "                X_train[i][j]=0\n",
    "                \n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        for j in range(len(X_test[i])):\n",
    "            if np.isnan(X_test[i][j]) or not np.isfinite(X_test[i][j]):\n",
    "                X_test[i][j]=0\n",
    "\n",
    "        '''\n",
    "        .append(0.00)\n",
    "            print(\"added\")\n",
    "    for i in range(len(X_test)):\n",
    "        while len(X_test[i])!=15:\n",
    "            X_test[i].append(0.00)\n",
    "            # print(\"added\")\n",
    "    '''\n",
    "\n",
    "    print(X_train.shape)\n",
    "    #SVM\n",
    "    \n",
    "    print(np.any(np.isnan(X_train)))\n",
    "    print(np.all(np.isfinite(X_train)))\n",
    "\n",
    "    print(np.any(np.isnan(y_train)))\n",
    "    print(np.all(np.isfinite(y_train)))\n",
    "    print(np.any(np.isnan(X_test)))\n",
    "    print(np.all(np.isfinite(X_test)))\n",
    "    print(np.any(np.isnan(y_test)))\n",
    "    print(np.all(np.isfinite(y_test)))\n",
    "    # Train it on the entire training data set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7108059813787266\n",
      "0.7996402782553496\n",
      "0.9138504322592347\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "classifier = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "\n",
    "\n",
    "    #  LOSS\n",
    "from sklearn.metrics import f1_score,precision_score, recall_score\n",
    "print(precision_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 filled\n",
      "(38208, 15)\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7108059813787266\n",
      "0.7996402782553496\n",
      "0.9138504322592347\n"
     ]
    }
   ],
   "source": [
    "svmclassifier(loss_set,loss_target)\n",
    "# svmclassifier(goss_set,goss_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20, 5), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(20, 5), random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7151570019535903\n",
      "0.7998827542101897\n",
      "0.9073816576990509\n"
     ]
    }
   ],
   "source": [
    "y_pred= clf.predict(X_test)\n",
    "print('----mlp-----')\n",
    "print(precision_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(recall_score(y_test, y_pred))\n",
    "#0.7176001537131329 0.7997537407318183 0.9031497491082764\n",
    "#0.7178894665896399 0.7992924717926727 0.9015174415089777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----randomforest-----\n",
      "0.7241638293611056\n",
      "0.7922109933448361\n",
      "0.8743727706910103\n",
      "----adaboost------\n",
      "0.7247665408305186\n",
      "0.7957352820876393\n",
      "0.8821111178284263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clfrf=RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "clfrf.fit(X_train, y_train) \n",
    "y_pred= clfrf.predict(X_test)\n",
    "print('----randomforest-----')\n",
    "print(precision_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(recall_score(y_test, y_pred))\n",
    "print('----adaboost------')\n",
    "clfad=RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "clfad.fit(X_train, y_train) \n",
    "y_pred= clfad.predict(X_test)\n",
    "print(precision_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7150835322195704\n",
      "0.7991784694993465\n",
      "0.905688894262741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clfr = SVC(gamma=2, C=1)\n",
    "clfr.fit(X_train, y_train)\n",
    "y_pred= clfr.predict(X_test)\n",
    "print(precision_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(recall_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7346200912226054\n",
      "0.7652017729366436\n",
      "0.7984402394051145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clfk=KNeighborsClassifier(2)\n",
    "clfk.fit(X_train, y_train)\n",
    "y_pred= clfk.predict(X_test)\n",
    "print(precision_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(recall_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "    # human: X; bot: Y\n",
    "gos_set = np.array(goss_set)\n",
    "np.nan_to_num(gos_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "gos_set,goss_target, test_size=0.4, random_state=0)\n",
    "    \n",
    "for i in range(len(X_train)):\n",
    "     while len(X_train[i]) < 15:\n",
    "        X_train[i].append(0.00)\n",
    "        print(X_train[i])\n",
    "        print(\"0 filled\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    for j in range(15):\n",
    "        try:\n",
    "            if np.isnan(X_train[i][j]) or not np.isfinite(X_train[i][j]):\n",
    "                X_train[i][j]=0\n",
    "        except IndexError:\n",
    "            X_train[i].append(0.00)\n",
    "            \n",
    "                \n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    for j in range(15):\n",
    "        try:\n",
    "            if np.isnan(X_test[i][j]) or not np.isfinite(X_test[i][j]):\n",
    "                X_test[i][j]=0\n",
    "        except IndexError:\n",
    "            X_test[i].append(0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38217,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-344-da973adcd903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Train it on the entire training data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m '''\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Get predictions on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m         X, y = check_X_y(X, y, dtype=np.float64,\n\u001b[1;32m    148\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                          accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "    print(X_train.shape)\n",
    "    #SVM\n",
    "    classifier = svm.SVC()\n",
    "    '''\n",
    "    print(np.any(np.isnan(X_train)))\n",
    "    print(np.all(np.isfinite(X_train)))\n",
    "\n",
    "    print(np.any(np.isnan(y_train)))\n",
    "    print(np.all(np.isfinite(y_train)))\n",
    "    print(np.any(np.isnan(X_test)))\n",
    "    print(np.all(np.isfinite(X_test)))\n",
    "    print(np.any(np.isnan(y_test)))\n",
    "    print(np.all(np.isfinite(y_test)))\n",
    "    # Train it on the entire training data set\n",
    "    '''\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions on the test set\n",
    "    y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
